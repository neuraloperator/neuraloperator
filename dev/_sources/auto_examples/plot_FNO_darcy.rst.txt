
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_FNO_darcy.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_plot_FNO_darcy.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_FNO_darcy.py:


Training a TFNO on Darcy-Flow
=============================

In this example, we demonstrate how to use the small Darcy-Flow example we ship with the package
to train a Tensorized Fourier-Neural Operator

.. GENERATED FROM PYTHON SOURCE LINES 11-25

.. code-block:: Python



    import torch
    import matplotlib.pyplot as plt
    import sys
    from neuralop.models import TFNO
    from neuralop import Trainer
    from neuralop.data.datasets import load_darcy_flow_small
    from neuralop.utils import count_model_params
    from neuralop import LpLoss, H1Loss

    device = 'cpu'









.. GENERATED FROM PYTHON SOURCE LINES 26-27

Loading the Navier-Stokes dataset in 128x128 resolution

.. GENERATED FROM PYTHON SOURCE LINES 27-36

.. code-block:: Python

    train_loader, test_loaders, data_processor = load_darcy_flow_small(
            n_train=1000, batch_size=32, 
            test_resolutions=[16, 32], n_tests=[100, 50],
            test_batch_sizes=[32, 32],
            positional_encoding=True
    )
    data_processor = data_processor.to(device)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/neuraloperator/neuraloperator/neuralop/data/datasets/pt_dataset.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
      data = torch.load(
    Loading test db for resolution 16 with 100 samples 
    /home/runner/work/neuraloperator/neuraloperator/neuralop/data/datasets/pt_dataset.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
      data = torch.load(Path(root_dir).joinpath(f"{dataset_name}_test_{res}.pt").as_posix())
    Loading test db for resolution 32 with 50 samples 




.. GENERATED FROM PYTHON SOURCE LINES 37-38

We create a tensorized FNO model

.. GENERATED FROM PYTHON SOURCE LINES 38-47

.. code-block:: Python


    model = TFNO(n_modes=(16, 16), hidden_channels=32, projection_channels=64, factorization='tucker', rank=0.42)
    model = model.to(device)

    n_params = count_model_params(model)
    print(f'\nOur model has {n_params} parameters.')
    sys.stdout.flush()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Our model has 523257 parameters.




.. GENERATED FROM PYTHON SOURCE LINES 48-49

Create the optimizer

.. GENERATED FROM PYTHON SOURCE LINES 49-55

.. code-block:: Python

    optimizer = torch.optim.Adam(model.parameters(), 
                                    lr=8e-3, 
                                    weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)









.. GENERATED FROM PYTHON SOURCE LINES 56-57

Creating the losses

.. GENERATED FROM PYTHON SOURCE LINES 57-64

.. code-block:: Python

    l2loss = LpLoss(d=2, p=2)
    h1loss = H1Loss(d=2)

    train_loss = h1loss
    eval_losses={'h1': h1loss, 'l2': l2loss}









.. GENERATED FROM PYTHON SOURCE LINES 65-76

.. code-block:: Python



    print('\n### MODEL ###\n', model)
    print('\n### OPTIMIZER ###\n', optimizer)
    print('\n### SCHEDULER ###\n', scheduler)
    print('\n### LOSSES ###')
    print(f'\n * Train: {train_loss}')
    print(f'\n * Test: {eval_losses}')
    sys.stdout.flush()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ### MODEL ###
     TFNO(
      (fno_blocks): FNOBlocks(
        (convs): SpectralConv(
          (weight): ModuleList(
            (0-3): 4 x ComplexTuckerTensor(shape=(32, 32, 16, 9), rank=(26, 26, 13, 7))
          )
        )
        (fno_skips): ModuleList(
          (0-3): 4 x Flattened1dConv(
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
        )
      )
      (lifting): MLP(
        (fcs): ModuleList(
          (0): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
          (1): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
        )
      )
      (projection): MLP(
        (fcs): ModuleList(
          (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
          (1): Conv1d(64, 1, kernel_size=(1,), stride=(1,))
        )
      )
    )

    ### OPTIMIZER ###
     Adam (
    Parameter Group 0
        amsgrad: False
        betas: (0.9, 0.999)
        capturable: False
        differentiable: False
        eps: 1e-08
        foreach: None
        fused: None
        initial_lr: 0.008
        lr: 0.008
        maximize: False
        weight_decay: 0.0001
    )

    ### SCHEDULER ###
     <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7efd6017d160>

    ### LOSSES ###

     * Train: <neuralop.losses.data_losses.H1Loss object at 0x7efd60db99d0>

     * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7efd60db99d0>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7efd60db9880>}




.. GENERATED FROM PYTHON SOURCE LINES 77-78

Create the trainer

.. GENERATED FROM PYTHON SOURCE LINES 78-87

.. code-block:: Python

    trainer = Trainer(model=model, n_epochs=20,
                      device=device,
                      data_processor=data_processor,
                      wandb_log=False,
                      eval_interval=3,
                      use_distributed=False,
                      verbose=True)









.. GENERATED FROM PYTHON SOURCE LINES 88-89

Actually train the model on our small Darcy-Flow dataset

.. GENERATED FROM PYTHON SOURCE LINES 89-99

.. code-block:: Python


    trainer.train(train_loader=train_loader,
                  test_loaders=test_loaders,
                  optimizer=optimizer,
                  scheduler=scheduler, 
                  regularizer=False, 
                  training_loss=train_loss,
                  eval_losses=eval_losses)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Training on 32 samples
    Testing on [50, 50] samples         on resolutions [16, 32].
    Raw outputs of shape torch.Size([32, 1, 16, 16])
    [0] time=2.58, avg_loss=0.7907, train_err=24.7086
    Eval: 16_h1=0.6247, 16_l2=0.4526, 32_h1=0.6672, 32_l2=0.4546
    [3] time=2.61, avg_loss=0.2376, train_err=7.4239
    Eval: 16_h1=0.6677, 16_l2=0.4887, 32_h1=0.7203, 32_l2=0.4912
    [6] time=2.63, avg_loss=0.1853, train_err=5.7915
    Eval: 16_h1=0.6814, 16_l2=0.4655, 32_h1=0.7398, 32_l2=0.4703
    [9] time=2.61, avg_loss=0.1761, train_err=5.5035
    Eval: 16_h1=0.7154, 16_l2=0.4946, 32_h1=0.7777, 32_l2=0.4999
    [12] time=2.56, avg_loss=0.1608, train_err=5.0244
    Eval: 16_h1=0.6615, 16_l2=0.4685, 32_h1=0.7252, 32_l2=0.4764
    [15] time=2.53, avg_loss=0.1409, train_err=4.4040
    Eval: 16_h1=0.6392, 16_l2=0.4759, 32_h1=0.7054, 32_l2=0.4857
    [18] time=2.53, avg_loss=0.1462, train_err=4.5689
    Eval: 16_h1=0.7197, 16_l2=0.4824, 32_h1=0.8009, 32_l2=0.4877

    {'train_err': 4.209666986018419, 'avg_loss': 0.13470934355258943, 'avg_lasso_loss': None, 'epoch_train_time': 2.5261761579999984}



.. GENERATED FROM PYTHON SOURCE LINES 100-110

Plot the prediction, and compare with the ground-truth 
Note that we trained on a very small resolution for
a very small number of epochs
In practice, we would train at larger resolution, on many more samples.

However, for practicity, we created a minimal example that
i) fits in just a few Mb of memory
ii) can be trained quickly on CPU

In practice we would train a Neural Operator on one or multiple GPUs

.. GENERATED FROM PYTHON SOURCE LINES 110-148

.. code-block:: Python


    test_samples = test_loaders[32].dataset

    fig = plt.figure(figsize=(7, 7))
    for index in range(3):
        data = test_samples[index]
        data = data_processor.preprocess(data, batched=False)
        # Input x
        x = data['x']
        # Ground-truth
        y = data['y']
        # Model prediction
        out = model(x.unsqueeze(0))

        ax = fig.add_subplot(3, 3, index*3 + 1)
        ax.imshow(x[0], cmap='gray')
        if index == 0: 
            ax.set_title('Input x')
        plt.xticks([], [])
        plt.yticks([], [])

        ax = fig.add_subplot(3, 3, index*3 + 2)
        ax.imshow(y.squeeze())
        if index == 0: 
            ax.set_title('Ground-truth y')
        plt.xticks([], [])
        plt.yticks([], [])

        ax = fig.add_subplot(3, 3, index*3 + 3)
        ax.imshow(out.squeeze().detach().numpy())
        if index == 0: 
            ax.set_title('Model prediction')
        plt.xticks([], [])
        plt.yticks([], [])

    fig.suptitle('Inputs, ground-truth output and prediction.', y=0.98)
    plt.tight_layout()
    fig.show()



.. image-sg:: /auto_examples/images/sphx_glr_plot_FNO_darcy_001.png
   :alt: Inputs, ground-truth output and prediction., Input x, Ground-truth y, Model prediction
   :srcset: /auto_examples/images/sphx_glr_plot_FNO_darcy_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 52.617 seconds)


.. _sphx_glr_download_auto_examples_plot_FNO_darcy.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_FNO_darcy.ipynb <plot_FNO_darcy.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_FNO_darcy.py <plot_FNO_darcy.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_FNO_darcy.zip <plot_FNO_darcy.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
