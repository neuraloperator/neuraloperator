{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a TFNO on Darcy-Flow\n\nIn this example, we demonstrate how to use the small Darcy-Flow example we ship with the package\nto train a Tensorized Fourier-Neural Operator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nimport sys\nfrom neuralop.models import TFNO\nfrom neuralop import Trainer\nfrom neuralop.training import AdamW\nfrom neuralop.data.datasets import load_darcy_flow_small\nfrom neuralop.utils import count_model_params\nfrom neuralop import LpLoss, H1Loss\n\ndevice = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the Navier-Stokes dataset in 128x128 resolution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loader, test_loaders, data_processor = load_darcy_flow_small(\n        n_train=1000, batch_size=32, \n        test_resolutions=[16, 32], n_tests=[100, 50],\n        test_batch_sizes=[32, 32],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a tensorized FNO model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = TFNO(n_modes=(16, 16), in_channels=1, hidden_channels=32, projection_channels=64, factorization='tucker', rank=0.42)\nmodel = model.to(device)\n\nn_params = count_model_params(model)\nprint(f'\\nOur model has {n_params} parameters.')\nsys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the optimizer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), \n                                lr=8e-3, \n                                weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the losses\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l2loss = LpLoss(d=2, p=2)\nh1loss = H1Loss(d=2)\n\ntrain_loss = h1loss\neval_losses={'h1': h1loss, 'l2': l2loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('\\n### MODEL ###\\n', model)\nprint('\\n### OPTIMIZER ###\\n', optimizer)\nprint('\\n### SCHEDULER ###\\n', scheduler)\nprint('\\n### LOSSES ###')\nprint(f'\\n * Train: {train_loss}')\nprint(f'\\n * Test: {eval_losses}')\nsys.stdout.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the trainer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model, n_epochs=20,\n                  device=device,\n                  data_processor=data_processor,\n                  wandb_log=False,\n                  eval_interval=3,\n                  use_distributed=False,\n                  verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Actually train the model on our small Darcy-Flow dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.train(train_loader=train_loader,\n              test_loaders={},\n              optimizer=optimizer,\n              scheduler=scheduler, \n              regularizer=False, \n              training_loss=train_loss, \n              save_every=1,\n              save_dir=\"./checkpoints\")\n\n\n# resume training from saved checkpoint at epoch 10\n\ntrainer = Trainer(model=model, n_epochs=20,\n                  device=device,\n                  data_processor=data_processor,\n                  wandb_log=False,\n                  eval_interval=3,\n                  use_distributed=False,\n                  verbose=True)\n\ntrainer.train(train_loader=train_loader,\n              test_loaders={},\n              optimizer=optimizer,\n              scheduler=scheduler, \n              regularizer=False, \n              training_loss=train_loss,\n              resume_from_dir=\"./checkpoints\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}